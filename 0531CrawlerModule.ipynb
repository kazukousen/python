{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cording: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://52.25.113.156')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "  <title>Comicomu</t\n"
     ]
    }
   ],
   "source": [
    "contents = r.text\n",
    "print contents[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "### document\n",
    "[Beautiful Soup 4.4.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/tops/about\n",
      "/users/sign_up\n",
      "/users/sign_in\n",
      "/users/auth/twitter\n",
      "/posts/5\n",
      "/posts/5\n",
      "/posts/4\n",
      "/posts/4\n",
      "/posts/3\n",
      "/posts/3\n",
      "/posts/1\n",
      "/posts/1\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'http://52.25.113.156',\n",
       " u'http://52.25.113.156/',\n",
       " u'http://52.25.113.156/posts/1',\n",
       " u'http://52.25.113.156/posts/3',\n",
       " u'http://52.25.113.156/posts/4',\n",
       " u'http://52.25.113.156/posts/5',\n",
       " u'http://52.25.113.156/tops/about',\n",
       " u'http://52.25.113.156/users/auth/twitter',\n",
       " u'http://52.25.113.156/users/sign_in',\n",
       " u'http://52.25.113.156/users/sign_up'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = u'http://52.25.113.156'\n",
    "to_crawl = {seed}\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    url = requests.compat.urljoin(seed, link.get('href'))\n",
    "    if url.find(\"'\")!=-1: continue\n",
    "    url = url.split('#')[0] # アンカーを取り除く\n",
    "    if url[0:4] == 'http':\n",
    "        to_crawl.add(url)\n",
    "    \n",
    "to_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gettextonly(soup):\n",
    "    v = soup.string\n",
    "    if v==None:\n",
    "        c = soup.contents\n",
    "        resulttext = ''\n",
    "        for t in c:\n",
    "            subtext = gettextonly(t)\n",
    "            resulttext += subtext + '\\n'\n",
    "        return resulttext\n",
    "    else:\n",
    "        return v.strip()\n",
    "\n",
    "textonly = gettextonly(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 html\n",
      "2 Comicomu\n",
      "3 ComiComu\n",
      "4 About\n",
      "5 Sign\n",
      "6 up\n",
      "7 Log\n",
      "8 in\n",
      "9 Log\n",
      "10 in\n",
      "11 with\n",
      "12 Twitter\n",
      "13 ComiComu\n",
      "14 みんな\n",
      "15 で\n",
      "16 勉強\n",
      "17 会\n",
      "18 を\n",
      "19 開こ\n",
      "20 う\n",
      "21 OSI\n",
      "22 参照\n",
      "23 モデル\n",
      "24 勉強\n",
      "25 会\n",
      "26 ネットワーク\n",
      "27 (\n",
      "28 その\n",
      "29 プロトコル\n",
      "30 通信\n",
      "31 )\n",
      "32 の\n",
      "33 理解\n",
      "34 の\n",
      "35 ため\n",
      "36 に\n",
      "37 \r\n",
      "38 O\n",
      "39 ...\n",
      "40 詳細\n",
      "41 1\n",
      "42 人\n",
      "43 が\n",
      "44 参加\n",
      "45 tag\n",
      "46 :\n",
      "47 div\n",
      "48 の\n",
      "49 閉じ\n",
      "50 就活\n",
      "51 必勝\n",
      "52 法\n",
      "53 面接\n",
      "54 で\n",
      "55 聞か\n",
      "56 れる\n",
      "57 事\n",
      "58 とか\n",
      "59 シェア\n",
      "60 しよ\n",
      "61 う\n",
      "62 詳細\n",
      "63 1\n",
      "64 人\n",
      "65 が\n",
      "66 参加\n",
      "67 tag\n",
      "68 :\n",
      "69 div\n",
      "70 の\n",
      "71 閉じ\n",
      "72 asdfa\n",
      "73 asdf\n",
      "74 詳細\n",
      "75 3\n",
      "76 人\n",
      "77 が\n",
      "78 参加\n",
      "79 tag\n",
      "80 :\n",
      "81 div\n",
      "82 の\n",
      "83 閉じ\n",
      "84 JSON\n",
      "85 Schema\n",
      "86 を\n",
      "87 書く\n",
      "88 JSON\n",
      "89 を\n",
      "90 返す\n",
      "91 RESTful\n",
      "92 な\n",
      "93 WEB\n",
      "94 API\n",
      "95 の\n",
      "96 \r\n",
      "97 仕\n",
      "98 ...\n",
      "99 詳細\n",
      "100 1\n",
      "101 人\n",
      "102 が\n",
      "103 参加\n",
      "104 tag\n",
      "105 :\n",
      "106 div\n",
      "107 の\n",
      "108 閉じ\n",
      "109 \n"
     ]
    }
   ],
   "source": [
    "def split_to_word(text):\n",
    "    words = []\n",
    "    m = MeCab.Tagger('-Ochasen')\n",
    "    text = text.encode('utf-8')\n",
    "    node = m.parseToNode(text)\n",
    "    while node:\n",
    "        words.append(node.surface)\n",
    "        node = node.next\n",
    "    return words\n",
    "\n",
    "text = u\"欲望が味方する Wow Wow Wow~\"\n",
    "for i, word in  enumerate(split_to_word(textonly)):\n",
    "    print i, word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## class Crawler\n",
    "\n",
    "### description\n",
    "オライリーの集合知プログラミングの第4章と[Pythonでつくる検索エンジン](http://nwpct1.hatenablog.com/entry/python-search-engine)を参考にしていく。   \n",
    "コード規約は、[Google Python Style Guide](http://works.surgo.jp/translation/pyguide.html)をみてコーディングを意識したい。\n",
    "\n",
    "### collection\n",
    "\n",
    "- url_list\n",
    "  - url\n",
    "- word_list\n",
    "  - word\n",
    "- word_location\n",
    "  - url_id\n",
    "  - word_id\n",
    "  - location\n",
    "- link\n",
    "  - row_id\n",
    "  - from_id\n",
    "  - to_id\n",
    "- link_words\n",
    "  - word_id\n",
    "  - link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    # データベースの名前でクローラを初期化する\n",
    "    def __init__(self, dbname):\n",
    "        client = MongoClient()\n",
    "        db = client[dbname]\n",
    "        self.db = db\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def dbcommit(self):\n",
    "        pass\n",
    "    \n",
    "    # エントリIDを取得したり、それが存在しない場合には追加\n",
    "    # するための補助関数\n",
    "    def get_entry_id(self, collection, key, value, createnew=True):\n",
    "        if object = self.db[collection].find({key: value}):\n",
    "            return object\n",
    "            object = self.db[collection].insert_one({key: value})\n",
    "        return \n",
    "    \n",
    "    # 個々のページをインデックスする\n",
    "    def add_to_index(self, url, soup):\n",
    "        if seld.is_indexed(url): return\n",
    "        print 'Indexing %s' % url\n",
    "        \n",
    "        # 個々の単語を取得する\n",
    "        text = self.get_text_only(soup)\n",
    "        words = self.separate_words(text)\n",
    "        \n",
    "        # URL idを取得する\n",
    "        url_id = self.get_entry_id('url_list', 'url', url)\n",
    "        \n",
    "        # それぞれの単語と、このurlのリング\n",
    "        for i, word in enumerate(words):\n",
    "            word_id = self.get_entry_id('word_list', 'word', word)\n",
    "            self.db['word_location'].insert_one({'url_id': url_id})\n",
    "    \n",
    "    # HTMLのページからタグのない状態でテキストを摘出する\n",
    "    def get_text_only(self, soup):\n",
    "        values = soup.string\n",
    "        if values == None:\n",
    "            contents = soup.contents\n",
    "            result_text = ''\n",
    "            for t in contents:\n",
    "                sub_text = self.get_text_only(t)\n",
    "                result_text += sub_text + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return values.strip()\n",
    "    \n",
    "    # 空白以外の文字で単語を分割する\n",
    "    def separate_words(self, text):\n",
    "        words = []\n",
    "        m = MeCab.Tagger('-Ochasen')\n",
    "        text = text.encode('utf-8')\n",
    "        node = m.parseToNode(text)\n",
    "        while node:\n",
    "            words.append(node.surface)\n",
    "            node = node.next\n",
    "        return words\n",
    "    \n",
    "    # URLが既にインデックスされていたらtrueを返す\n",
    "    def is_indexed(self, url):\n",
    "        return False\n",
    "    \n",
    "    # ２つのページ間にリンクを付け加える\n",
    "    def add_link_ref(self, url_from, url_to, link_text):\n",
    "        pass\n",
    "    \n",
    "    # ページを受け取り、与えられた深さで幅優先の検索を行い\n",
    "    # ページをインデクシングする\n",
    "    def crawl(self, seed, max_depth=2):\n",
    "        pages = {seed}\n",
    "        crawled = []\n",
    "        next_depth = []\n",
    "        depth = 0\n",
    "        while pages and depth <= max_depth:\n",
    "            page_url = pages.pop()\n",
    "            if page_url not in crawled:\n",
    "                res = requests.get(page_url)\n",
    "                try:\n",
    "                    res.status_code == 200\n",
    "                except:\n",
    "                    print 'Could not open %s' % page\n",
    "                    continue\n",
    "                html_doc = res.text\n",
    "                soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                self.add_to_index(page_url, soup)\n",
    "\n",
    "                for link in soup.find_all('a'):\n",
    "                    link_url = urljoin(page_url, link.get('href'))\n",
    "                    if link_url.find(\"'\")!=-1: continue\n",
    "                    if link_url[-1] == '/':\n",
    "                        link_url = link_url[:-1] # 最後が'/'で終わる場合取り除く\n",
    "                    link_url = link_url.split('#')[0] # アンカーを取り除く\n",
    "                    link_url = link_url.split('?')[0] # パラメータを取り除く\n",
    "                    if url[0:4] == 'http':\n",
    "                        pages.add(link_url)\n",
    "                    link_text = self.get_text_only(link)\n",
    "                    self.add_link_ref(page_url, link_url, link_text)\n",
    "\n",
    "                crawled.append(page_url)\n",
    "\n",
    "            if not pages:\n",
    "                pages, next_depth = next_depth, []\n",
    "                depth += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler = Crawler('Crawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler.crawl('http://52.25.113.156', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'test/'\n",
    "if s[-1] == '/':\n",
    "    s = s[:-1]\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'Crawler'), u'word_list')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.db['word_list']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
