{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cording: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko)     Chrome/48.0.2564.109 Safari/537.36 Vivaldi/1.0.403.24'}\n",
    "r= requests.get('http://blog.nogizaka46.com/asuka.saito/?d=20160505', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contents = r.text\n",
    "print contents[100:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "### document\n",
    "[Beautiful Soup 4.4.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag = soup.find(id='sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tag.a.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag2 = soup.find(id='daytable')\n",
    "tag2.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for link in tag2.find_all('a'):\n",
    "    print link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag3 = soup.find(id='calendarPN')\n",
    "prev = tag3.a.get('href')\n",
    "prev[-4:] == '1111' # 一番初期かどうか\n",
    "print tag3.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# あすかの\n",
    "\n",
    "## Database\n",
    "\n",
    "DBname : Asuka\n",
    "\n",
    "- Links\n",
    "  - month : YYmm\n",
    "  - url : []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['Asuka']\n",
    "col = db.Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers ={'User-Agent': 'sample Vivaldi'}\n",
    "r= requests.get('http://blog.nogizaka46.com/asuka.saito/?d=201112', headers=headers)\n",
    "r.status_code\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = ['http://blog.nogizaka46.com/asuka.saito/?d=201605']\n",
    "crawled = []\n",
    "# カレンダー判定\n",
    "def add_link(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    tag = soup.find(id='calendarPN')\n",
    "    prev = tag.a.get('href')\n",
    "    if prev[-4:] == '1111':\n",
    "        col.insert_one({'month': prev[-4:], 'day_url': []})\n",
    "        add_day_link(text, prev[-4:])\n",
    "        crawled.append(prev)\n",
    "        return \n",
    "    else:\n",
    "        links.append(prev)\n",
    "\n",
    "def add_day_link(text, month):\n",
    "    day_add = col.find_one({'month': month})\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    tag = soup.find(id='daytable')\n",
    "    for day_link in tag.find_all('a'):\n",
    "        day_add['day_url'].append(day_link.get('href'))\n",
    "        col.save(day_add)\n",
    "\n",
    "while links:\n",
    "    url = links.pop()\n",
    "    month = url[-4:]\n",
    "    crawled.append(url)\n",
    "    col.insert_one({'month': month, 'day_url': []})\n",
    "    r = requests.get(url, headers=headers)\n",
    "    text = r.text\n",
    "    add_day_link(text, month)\n",
    "    add_link(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_1111 = col.find_one({'month': '1112'})\n",
    "for day_url in month_1111['day_url']:\n",
    "    print day_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def union(p, q):\n",
    "    for e in q:\n",
    "        if e not in p:\n",
    "            p.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_link= []\n",
    "for a in col.find({'month': re.compile(r'.*')}):\n",
    "    union(all_link, a['day_url'])\n",
    "    \n",
    "all_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = u'http://52.25.113.156'\n",
    "to_crawl = [seed]\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    url = requests.compat.urljoin(seed, link.get('href'))\n",
    "    if url.find(\"'\")!=-1: continue\n",
    "    url = url.split('#')[0] # アンカーを取り除く\n",
    "    if url[0:4] == 'http':\n",
    "        to_crawl.add(url)\n",
    "    \n",
    "to_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gettextonly(soup):\n",
    "    v = soup.string\n",
    "    if v==None:\n",
    "        c = soup.contents\n",
    "        resulttext = ''\n",
    "        for t in c:\n",
    "            subtext = gettextonly(t)\n",
    "            resulttext += subtext + '\\n'\n",
    "        return resulttext\n",
    "    else:\n",
    "        return v.strip()\n",
    "\n",
    "textonly = gettextonly(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_to_word(text):\n",
    "    words = []\n",
    "    m = MeCab.Tagger('-Ochasen')\n",
    "    text = text.encode('utf-8')\n",
    "    node = m.parseToNode(text)\n",
    "    while node:\n",
    "        words.append(node.surface)\n",
    "        node = node.next\n",
    "    return words\n",
    "\n",
    "text = u\"欲望が味方する Wow Wow Wow~\"\n",
    "for i, word in  enumerate(split_to_word(textonly)):\n",
    "    print i, word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## class Crawler\n",
    "\n",
    "### description\n",
    "オライリーの集合知プログラミングの第4章と[Pythonでつくる検索エンジン](http://nwpct1.hatenablog.com/entry/python-search-engine)を参考にしていく。   \n",
    "コード規約は、[Google Python Style Guide](http://works.surgo.jp/translation/pyguide.html)をみてコーディングを意識したい。\n",
    "\n",
    "### collection\n",
    "\n",
    "- url_list\n",
    "  - url\n",
    "- word_list\n",
    "  - word\n",
    "- word_location\n",
    "  - url_id\n",
    "  - word_id\n",
    "  - location\n",
    "- link\n",
    "  - row_id\n",
    "  - from_id\n",
    "  - to_id\n",
    "- link_words\n",
    "  - word_id\n",
    "  - link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    # データベースの名前でクローラを初期化する\n",
    "    def __init__(self, dbname):\n",
    "        client = MongoClient()\n",
    "        db = client[dbname]\n",
    "        self.col = db.Index\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def dbcommit(self):\n",
    "        pass\n",
    "    \n",
    "    # エントリIDを取得したり、それが存在しない場合には追加\n",
    "    # するための補助関数\n",
    "    def get_entry_id(self, collection, key, value, createnew=True):\n",
    "        return None\n",
    "    \n",
    "    # 個々のページをインデックスする\n",
    "    def add_page_to_index(self, url, soup):\n",
    "        if self.is_indexed(url): return\n",
    "        print 'Indexing %s' % url\n",
    "\n",
    "        # 個々の単語を取得する\n",
    "        text = self.get_text_only(soup)\n",
    "        words = self.separate_words(text)\n",
    "\n",
    "        for word in words:\n",
    "            self.add_to_index(word, url)\n",
    "\n",
    "    def add_to_index(self, keyword, url):\n",
    "        entry = self.col.find_one({'keyword': keyword})\n",
    "        if entry:\n",
    "            if url not in entry['url']:\n",
    "                entry['url'].append(url)\n",
    "                self.col.save(entry)\n",
    "                return\n",
    "        # 単語が見つからないから新規作成\n",
    "        self.col.insert_one({'keyword': keyword, 'url': [url]})\n",
    "        self.col.create_index('keyword')\n",
    "\n",
    "    # HTMLのページからタグのない状態でテキストを摘出する\n",
    "    def get_text_only(self, soup):\n",
    "        values = soup.string\n",
    "        if values == None:\n",
    "            contents = soup.contents\n",
    "            result_text = ''\n",
    "            for t in contents:\n",
    "                sub_text = self.get_text_only(t)\n",
    "                result_text += sub_text + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return values.strip()\n",
    "    \n",
    "    # 空白以外の文字で単語を分割する\n",
    "    def separate_words(self, text):\n",
    "        words = []\n",
    "        m = MeCab.Tagger('-Ochasen')\n",
    "        text = text.encode('utf-8')\n",
    "        node = m.parseToNode(text)\n",
    "        while node:\n",
    "            words.append(node.surface)\n",
    "            node = node.next\n",
    "        return words\n",
    "\n",
    "    # URLが既にインデックスされていたらtrueを返す\n",
    "    def is_indexed(self, url):\n",
    "        return False\n",
    "    \n",
    "    # ２つのページ間にリンクを付け加える\n",
    "    def add_link_ref(self, url_from, url_to, link_text):\n",
    "        pass\n",
    "    \n",
    "    def union(self, p, q):\n",
    "        for e in q:\n",
    "            if e not in p:\n",
    "                p.append(e)\n",
    "    \n",
    "    # ページを受け取り、与えられた深さで幅優先の検索を行い\n",
    "    # ページをインデクシングする\n",
    "    def crawl(self, seed, max_depth=2):\n",
    "        pages = [seed]\n",
    "        crawled = []\n",
    "        next_depth = []\n",
    "        depth = 0\n",
    "        while pages and depth <= max_depth:\n",
    "            page_url = pages.pop()\n",
    "            if page_url not in crawled:\n",
    "                res = requests.get(page_url)\n",
    "                try:\n",
    "                    res.status_code == 200\n",
    "                except:\n",
    "                    print 'Could not open %s' % page\n",
    "                    continue\n",
    "                html_doc = res.text\n",
    "                soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                self.add_page_to_index(page_url, soup)\n",
    "                links = []\n",
    "                for link in soup.find_all('a'):\n",
    "                    link_url = urljoin(page_url, link.get('href'))\n",
    "                    if link_url.find(\"'\")!=-1: continue\n",
    "                    if link_url[-1] == '/':\n",
    "                        link_url = link_url[:-1] # 最後が'/'で終わる場合取り除く\n",
    "                    link_url = link_url.split('#')[0] # アンカーを取り除く\n",
    "                    link_url = link_url.split('?')[0] # パラメータを取り除く\n",
    "                    if link_url[0:4] == 'http':\n",
    "                        links.append(link_url)\n",
    "                    link_text = self.get_text_only(link)\n",
    "                    self.add_link_ref(page_url, link_url, link_text)\n",
    "                \n",
    "                self.union(next_depth, links)\n",
    "\n",
    "                crawled.append(page_url)\n",
    "                print pages\n",
    "                print next_depth\n",
    "\n",
    "            if not pages:\n",
    "                pages, next_depth = next_depth, []\n",
    "                depth += 1\n",
    "                print depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler = Crawler('Crawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler.crawl('http://52.25.113.156', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'test/'\n",
    "if s[-1] == '/':\n",
    "    s = s[:-1]\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler.db['word_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def union(p,q):\n",
    "    for e in q:\n",
    "        if e not in p:\n",
    "            p.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = ['http://52.25.113.156']\n",
    "type(pages)\n",
    "union(pages, [u'http://52.25.113.156/users/sign_up', u'http://52.25.113.156/users/sign_in', u'http://52.25.113.156/tops/about', u'http://52.25.113.156', u'http://52.25.113.156/posts/4', u'http://52.25.113.156/posts/5', u'http://52.25.113.156/users/auth/twitter', u'http://52.25.113.156/posts/3', u'http://52.25.113.156/posts/1'])\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
